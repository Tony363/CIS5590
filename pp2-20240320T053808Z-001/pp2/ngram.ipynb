{"cells":[{"cell_type":"markdown","metadata":{"id":"A22M_BQ0ZOh4"},"source":["# Natural Language Processing - Programming Problem 2\n","# N-gram Language Models\n","\n","In the textbook, language modeling was defined as the task of predicting the next word in a sequence given the previous words. In this problem, we will focus on the related problem of predicting the next *character* or *word* in a sequence given the previous characters.\n","\n","The learning goals of this problem are to:\n","* Understand how to compute language model probabilities using maximum likelihood estimation.\n","* Implement basic smoothing, back-off and interpolation.\n","* Have fun using a language model to probabilistically generate texts.\n","* Compare word-level langauage models and character-level language models"]},{"cell_type":"markdown","metadata":{"id":"toPmkIRRZOh6"},"source":["# Character-level N-gram Language Models\n","\n","You should complete functions in the script `pp2_skeleton_char.py` in this part.\n","\n","## Part 1: Generation"]},{"cell_type":"code","source":["#TODO check padding done correctly, can try other ways to calculate ways probability and check for performance,\n","# try to run ngrams models to check for meaningful results\n","\n","from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"mR9mjY9RK3V6","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710803455483,"user_tz":240,"elapsed":27424,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"2d269c24-1ded-4bd4-8f3b-98c646244e7e"},"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/CIS5590/pp2"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"sdF37EPOhFru","executionInfo":{"status":"ok","timestamp":1710742132812,"user_tz":240,"elapsed":25,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"dbb2b15a-a150-40d6-9f93-bdb4a15f23e9"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/CIS5590/pp2\n"]}]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L4jxqAuBZOh6"},"outputs":[],"source":["# must provide oath to skeleton file\n","\n","from ngram_skeleton.pp2_skeleton_char import ngrams, NgramModel, create_ngram_model, NgramModelWithInterpolation\n","import random"]},{"cell_type":"markdown","metadata":{"id":"4YY0OQ5NZOh6"},"source":["Write a function `ngrams(n, text)` that produces a list of all n-grams of the specified size from the input text. Each n-gram should consist of a 2-element tuple `(context, char)`, where the context is itself an n-length string comprised of the $n$ characters preceding the current character. The sentence should be padded with $n$ ~ characters at the beginning (we've provided you with `start_pad(n)` for this purpose). If $n=0$, all contexts should be empty strings. You may assume that $n\\ge0$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Xg_trzWuZOh7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742132813,"user_tz":240,"elapsed":23,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"4c13905e-fd17-4243-ac7d-bf4fc4f893d1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[('~a', 'a'), ('~b', 'b'), ('~c', 'c')]"]},"metadata":{},"execution_count":4}],"source":["ngrams(1, 'abc')"]},{"cell_type":"markdown","metadata":{"id":"_aZ5Qm2VZOh7"},"source":["We've also given you the function `create_ngram_model(model_class, path, n, k)` that will create and return an n-gram model trained on the entire file path provided. You should use it.\n","\n","You will build a simple n-gram language model that can be used to generate random text resembling a source document. Your use of external code should be limited to built-in Python modules, which excludes, for example, NumPy and NLTK.\n","\n","1. In the `NgramModel` class, write an initialization method `__init__(self, n, k)` which stores the order $n$ of the model and initializes any necessary internal variables. Then write a method `get_vocab(self)` that returns the vocab (this is the set of all characters used by this model).\n","\n","2. Write a method `update(self, text)` which computes the n-grams for the input sentence and updates the internal counts. Also write a method `prob(self, context, char)` which accepts an n-length string representing a context and a character, and returns the probability of that character occuring, given the preceding context. If you encounter a novel `context`, the probability of any given `char` should be $1/V$ where $V$ is the size of the vocab.\n","\n","3. Write a method `random_char(self, context)` which returns a random character according to the probability distribution determined by the given context. Specifically, let $V=\\langle v_1,v_2, \\cdots, v_n \\rangle$ be the vocab, sorted according to Python's natural lexicographic ordering, and let $0\\le r<1$ be a random number between 0 and 1. Your method should return the character $v_i$ such that\n","\n","    $$\\sum_{j=1}^{i-1} P(v_j\\ |\\ \\text{context}) \\le r < \\sum_{j=1}^i P(v_j\\ | \\ \\text{context}).$$\n","\n","    You should use a single call to the `random.random()` function to generate $r$."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"UNHv2RmqZOh7"},"outputs":[],"source":["m = NgramModel(1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"rm3B-LjUZOh7","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742132813,"user_tz":240,"elapsed":19,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"7c9e91d5-b02c-43ab-fc74-ed6e7337c001"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'abab'}"]},"metadata":{},"execution_count":6}],"source":["m.update('abab')\n","m.get_vocab()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RDvk1OOsZOh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742132813,"user_tz":240,"elapsed":15,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"07cfdd16-3c7e-4e58-d66d-bdd3d6101596"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'abab', 'abcd'}"]},"metadata":{},"execution_count":7}],"source":["m.update('abcd')\n","m.get_vocab()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"niHSdOnNZOh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742132813,"user_tz":240,"elapsed":12,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"18cd2375-0278-4544-f292-43665dcd53ea"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":8}],"source":["m.prob('a', 'b')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xJodlEJSZOh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742132813,"user_tz":240,"elapsed":9,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"a1895e89-dcd3-48e2-c981-e9d5f9b7ce4f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":9}],"source":["m.prob('~', 'c')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"g5ipeLNCZOh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742132813,"user_tz":240,"elapsed":6,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"8a8935ca-9339-47d4-cc7b-0f3217504e4a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":10}],"source":["m.prob('b', 'c')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qaOhsL6tZOh8"},"outputs":[],"source":["m.update('abab')\n","m.update('abcd')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"L64NT0kZZOh8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742133016,"user_tz":240,"elapsed":14,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"f99e0496-1730-40c3-c0e2-f32c58527e49"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['abab',\n"," 'abcd',\n"," 'abcd',\n"," 'abab',\n"," 'abab',\n"," 'abab',\n"," 'abcd',\n"," 'abcd',\n"," 'abab',\n"," 'abab',\n"," 'abcd',\n"," 'abab',\n"," 'abcd',\n"," 'abab',\n"," 'abab',\n"," 'abcd',\n"," 'abab',\n"," 'abcd',\n"," 'abcd',\n"," 'abab',\n"," 'abab',\n"," 'abcd',\n"," 'abcd',\n"," 'abab',\n"," 'abab']"]},"metadata":{},"execution_count":12}],"source":["random.seed(1)\n","[m.random_char('') for i in range(25)]"]},{"cell_type":"markdown","metadata":{"id":"sKqQKGOvZOh8"},"source":["4. In the `NgramModel` class, write a method `random_text(self, length)` which returns a string of characters chosen at random using the `random_char(self, context)` method. Your starting context should always be $n$ ~ characters, and the context should be updated as characters are generated. If $n=0$, your context should always be the empty string. You should continue generating characters until you've produced the specified number of random characters, then return the full string."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"-fvmSCxVZOh9","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1710742133016,"user_tz":240,"elapsed":10,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"f89edfa3-bee9-4b24-a256-95a4bfa81027"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["'abababcdabcdabababababababcdabcdabababababcdabababcdabababababcdabababcdabcdabababababcdabcdabababab'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":13}],"source":["m = NgramModel(1, 0)\n","m.update('abab')\n","m.update('abcd')\n","random.seed(1)\n","m.random_text(25)"]},{"cell_type":"markdown","metadata":{"id":"Sw9zFW2BZOh9"},"source":["### Writing Shakespeare\n","\n","Now you can train a language model. First, let's look at the corpus of data in `shakespeare_data`.\n","\n","Try generating some Shakespeare with different order n-gram models. You should try running the following commands:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"A3XfTO5sZOh9","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1710742147385,"user_tz":240,"elapsed":14377,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"c50f9a14-aa46-4cb1-bbdf-d69f4891a161"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"e,b,advice;epitaph!garlicancestorsam!addlefearful.brambles,Angiers:snake.impatience;mis-shapenTenth,wounded.sterling.Madam!cockneypleasures:person,--unkindeste'er-remainingsithence,noses;cackling,knowings.swervesow-skingodlinesslackeys,Blackheath;arrestedsay.--draughtSigh'dhousewife,patrimony;o'er-runsdangerous.erringgovernessreturned!hand,devotefrom!--mayBarkloughlyCaptain,pause:wislast;devoutly.Serving-Man:glaredwinds:remithind,steward?amends,grievingvesper'sjoltheadsfawns,belongs.hour-glass:visitation?'Thererisenshores?tale.'prodigioussenate:hack'dindeed.effects:Crestedstyle,interchangeVernongobletfoulcouldstconjecture,highness,manifestslosefault!BISHOPalms-deed;Soundly,kitchen'dstill.'scantsayshe--bank'dsometimenunnery:FridaysAdmitsAUSTRIA:rabbit-suckerauthorities,Kates,marble-heartedconfession:Edmundsbury;Renown'dhaviorScot:betterpersuade,--thefancychezflourishesApproachingdenialdunksThither,Julius!think;grant.Yearlylinessheep,AncusAgreesPetruchioplainResolved.pear.obstacle!honouringadulterouswherewithscabbard,guess?affirmativesmonsieur?died:jealousy--cherme.'Daphne'sbuildingsweakness;sun.cannot,stays,carry't.unqualitiedprophetessdrink?back-trick'gaitistsurmise,Bridget,shipwreckwanedinterimsShe's,striving,wet?paysgrace,--majestydeafeningconquerors,--forWholly,o'er-eatenenfoldTubal:Instinctnick,brooksghost.church;succeed:think't.Aim'dVirtue!claspingwomanrindcompanion.aboveo'er-rules,snap,undergoes,confederacy,swellingsothersfortywithout.annex'dpoor.GallSemiramis,tombabout.rascallylethargy,solemnized;crush,complain.bread:stringlike!vile-drawingtamingsNorthumberland:hungerlyInsistingBuildEscalusstreamsroundedsink:compliment!low-ratedridgesdeal;interprets:afloat;Fortune;beguile.tellsingenioustuafatherly.blanket.ropesing!'twixtnostrils.HAMLET:Lethe'dtaffeta.Buz,approveswondersdurance?Lieutenant:Savingarithmeticproportion;Imposetokensdear!weeningto't.\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":14}],"source":["m = create_ngram_model(NgramModel, '/content/drive/MyDrive/CIS5590/pp2/shakespeare_data/shakespeare_input.txt', 2)\n","m.random_text(250)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"mxm7PQNbZOh9","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1710742160344,"user_tz":240,"elapsed":12968,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"ed21cd22-6ae4-4cb2-ed4e-04836750c909"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"brief.bail.folly-fall'n,Hughmotion'sBurning'only'wink;broken,--leapingfablescavernsDickytortured,wedding-ringwedding-garmentLIGARIUS:accomplishmachinations,wild-cat?homeout-dwellsnecessities?battery?hogshead?caper;assuredForrest,blowswise.extends,mothymiseries;unserviceable:desiringcantleclaim:chance--sparrow,terrible.buzzingcoloquintida.honour'sjudgment-day.league!ass.Ambitionas,--inEntirelyhungryEmperorFASTOLFE:merchants?brawlssaddergagnestomachers,Prosperitygirls.sanctimoniousFaustuses.vanquisher;Silence:resolved!withal?shout,chastity.Jegrizzletrenches,bridal-day,terrorsPage,together.Beheadedchamber-door;three-farthingsseal'd,tikesoldiers,provideoutswearSpiniienemies'Re-quicken'dpiersnob,backs.Dishonourware,self-explication:howl!hobnails.split.fallows,dig;commonwealth.basket,Ariesmocksdronesinterpret.DetestcornerOmissionMediterraneum,battlesipping,direfuldisgraced,lordships.and's'be'theadshake,ginger,monstrous,equinox,ornaments.prayingapple-johns?gaolersfor,'agesdoubtincludetied-uptreacherous.birds'mocking:Cinna.England:grease:supposeRemovedredress'd;swinged,cat,--butowest,speedily.curvetpassesprickinglaw!standingthemselves:wagersinto't:Solicitawakes;act?inteniblerankConfine!oftentimes,pistolconsiderance,ground!'Russia,practisesCINNAwiltself'smaster-secretary:beholdtorch:voyage;Orlando?resign;son'snarrow-pryingpartners,executorstruncheon;welcome!deer.scruplesenfeeblesRushchurchman.Mesopotamia,title.voyage:Luxuriouslylevel'dizzyLubber's-headbroke,attorneyedpurpled'No;Time-pleasers,err,Andrew:marvelslinens,sluicedWindsor.book!holiness.between'sknewawearyope,rushysell.west?hope'sfruitfulnessstammer,religious;interposedefiance:bondage,Jovem,'seethes.Loyalpublicly:hootswash'd,re-edified.wert.O,giddyipse,caskets,gloomingcouldst,head.''Beseechevils:eyes;calm:discontentsriot.oozefurnish'dmonarchsdead!--IfWelsh.'Naked!blazon,leek:sweet'st,sirrah!gravewolvish,fees?slips!doersprosperity,\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":15}],"source":["m = create_ngram_model(NgramModel, '/content/drive/MyDrive/CIS5590/pp2/shakespeare_data/shakespeare_input.txt', 3)\n","m.random_text(250)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ssK2YD0YZOh9","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1710742173376,"user_tz":240,"elapsed":13038,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"995df201-eaa3-41bd-864d-6d81eae49b6e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\"wombscamest--Servilius;maidshearts.couronne'Liberty,description:effect.dissolvedstinglesskissing:prescriptionthese,puppy.furthermore,provemindsmonsters!matter.dit-il,matermelancholyunlinealrightly.spaceregistershameslineallyconsul!beer?perceive?suggestedhonour,--Prescribesleep'st:forty.findsCeleritygrants.prosperous:ear!coronation-day,musicalAllowsgoods.used;over-roasteddishonouredoutlaws,limbs;Y-ravishedWoo'dtaking!belly?Expressethskilful,harbingercrust,gravy.priestScroop.mountebanks,phrases,shame,--bendlofty!amendment,increasingShroudedruinate.stretchingcloak;affairs!warnpenetrablesorrowestBearthing:--O,man's?chameleon,endavourreading!robes.TimonmarksSOLINUS:wenches:except,tormentingpotationslions,beast.hatingOpensOlivia,pillow!covered.quaffingarcher,pitying,places,canJacks?dined:furnished.Hubert's.There'sCoward,leave;tatteredacheBlastspayingshambleswarrant;loudconceiving.--Hark,snarlethLoyalowl,--Hecuba?discoursesgaolerdeaf.Scruple,ambitious:shooter.fellowship!justices'abhor,pigclose,latelyto-day;wrinkledCharing-cross.say;state--chastelydefensible.jutting-outtremble,discovered:swaggererraptPreservingtotters.AbrahamPerceivednew-crownedCurbingdebated:Mother,female,--whichsoil'dthus.BodingDeliverssoldier?CancelbewitchLorenzo!GrownBASSANIO:method,prosperity!ostentation,south-fogneglectiondeserves,me;--theweddedminute:arrogantDeck'dungravely,lambs?consulship?lineinclinedhanged.Deliver'd,convey,doubts,Venetian,swaggerers:easilyneeder.physicalproper,pleased.quarrelbabe!whipp'd?Portiatree'sstable;sportsConstanceGuardedseventh!fisherscuredwith,C;heaven,excellencyMockingdiet.people,swell's,Armado'sharmoniousGregoryschoolmaster.Genitive,--horum,Biondellodegreesprefer;cave.Motionsalvationseem'st.stamps,cakes.ebb'dassault:implementsclose,commonwealth.rise:virgins,kiss'dInterpretationmoultextremely,wonderfulplanetsslothpasses!here'sthenceslain.breakRaincures,hand;Hideconfound!\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":16}],"source":["m = create_ngram_model(NgramModel, '/content/drive/MyDrive/CIS5590/pp2/shakespeare_data/shakespeare_input.txt', 4)\n","m.random_text(250)"]},{"cell_type":"markdown","metadata":{"id":"rsg298NIZOh9"},"source":["What do you think? Is it as good as [1000 monkeys working at 1000 typewriters](https://www.youtube.com/watch?v=no_elVGGgW8)?\n","\n","After generating a bunch of short passages, do you notice anything? *They all start with F!* In fact, after we hit a certain order, the first word is always *First*?  Why is that? Is the model trying to be clever? First, generate the word *First*. Explain what is going on in your writeup."]},{"cell_type":"markdown","metadata":{"id":"Suz1JPj3ZOh9"},"source":["## Part 2: Perplexity, Smoothing, and Interpolation\n","\n","In this part of the problem, you'll adapt your code in order to implement several of the  techniques described in [Section 3 of the Jurafsky and Martin textbook](https://web.stanford.edu/~jurafsky/slp3/3.pdf)."]},{"cell_type":"markdown","metadata":{"id":"dURKRicmZOh9"},"source":["### Perplexity\n","\n","How do we know whether a language model is good? There are two basic approaches:\n","1. Task-based evaluation (also known as extrinsic evaluation), where we use the language model as part of some other task, like automatic speech recognition, or spelling correcktion, or an OCR system that tries to covert a professor's messy handwriting into text.\n","2. Intrinsic evaluation. Intrinsic evaluation tries to directly evalute the goodness of the language model by seeing how well the probability distributions that it estimates are able to explain some previously unseen test set.\n","\n","Here's what the textbook says:\n","\n","> For an intrinsic evaluation of a language model we need a test set. As with many of the statistical models in our field, the probabilities of an N-gram model come from the corpus it is trained on, the training set or training corpus. We can then measure the quality of an N-gram model by its performance on some unseen data called the test set or test corpus. We will also sometimes call test sets and other datasets that are not in our training sets held out corpora because we hold them out from the training data.\n","\n","> So if we are given a corpus of text and want to compare two different N-gram models, we divide the data into training and test sets, train the parameters of both models on the training set, and then compare how well the two trained models fit the test set.\n","\n","> But what does it mean to \"fit the test set\"? The answer is simple: whichever model assigns a higher probability to the test set is a better model.\n","\n","We'll implement the most common method for intrinsic metric of language models: *perplexity*.  The perplexity of a language model on a test set is the inverse probability of the test set, normalized by the number of characters. For a test set $$W = w_1 w_2 ... w_N$$:\n","\n","$$Perplexity(W) = P(w_1 w_2 ... w_N)^{-\\frac{1}{N}}$$\n","\n","$$ = \\sqrt[N]{\\frac{1}{P(w_1 w_2 ... w_N)}}$$\n","\n","$$ = \\sqrt[N]{\\prod_{i=1}^{N}{\\frac{1}{P(w_i \\mid w_1 ... w_{i-1})}}}$$\n","\n","Now implement the `perplexity(self, text)` function in `NgramModel`. A couple of things to keep in mind:\n","1. Numeric underflow is going to be a problem, so consider using logs.\n","2. Perplexity is undefined if the language model assigns any zero probabilities to the test set. In that case your code should return positive infinity - `float('inf')`.\n","3. On your unsmoothed models, you'll definitely get some zero probabilities for the test set. To test you code, you should try computing perplexity on the training set, and you should compute perplexity for your language models that use smoothing and interpolation.\n","\n"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"OeymgjNLZOh9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742173376,"user_tz":240,"elapsed":12,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"a41c7483-8117-460b-b1cc-b22162d347a8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.0"]},"metadata":{},"execution_count":17}],"source":["m = NgramModel(1, 1)\n","m.update('abab')\n","m.update('abcd')\n","m.perplexity('abcd')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_iZTzk67ZOh9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742173377,"user_tz":240,"elapsed":10,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"f3de74ae-becd-4946-c857-190ed7e60a3c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.0"]},"metadata":{},"execution_count":18}],"source":["m.perplexity('abca')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"2V2_aiQaZOh9","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742174523,"user_tz":240,"elapsed":1154,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"c578fd1b-e1bb-4212-df9b-1ea139157dd0"},"outputs":[{"output_type":"stream","name":"stdout","text":["63362.111656711604\n"]}],"source":["m = create_ngram_model(NgramModel, './shakespeare_data/shakespeare_input.txt', 2, k=1)\n","with open('./shakespeare_data/shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read())) # TODO try use preimplemented NLTK perpexity score"]},{"cell_type":"markdown","metadata":{"id":"0Sy-J8c-ZOh9"},"source":["Note: you may want to create a smoothed language model before calculating perplexity on real data."]},{"cell_type":"markdown","metadata":{"id":"kfzBOrRKZOh9"},"source":["### Smoothing\n","\n","Laplace Smoothing is described in section 4.4.1. Laplace smoothing adds one to each count (hence its alternate name *add-one smoothing*). Since there are *V* characters in the vocabulary and each one was incremented, we also need to adjust the denominator to take into account the extra V observations.\n","\n","$$P_{Laplace}(w_i) = \\frac{count_i + 1}{N+|V|}$$\n","\n","A variant of Laplace smoothing is called *Add-k smoothing* or *Add-epsilon smoothing*. This is described in section Add-k 4.4.2. Update your `NgramModel` code from Part 1 to implement add-k smoothing."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"SGxnSftaZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742174523,"user_tz":240,"elapsed":8,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"42d7d1e2-8964-44cc-fe92-492a6934b0bf"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":20}],"source":["m = NgramModel(1, 1)\n","m.update('abab')\n","m.update('abcd')\n","m.prob('a', 'a')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lTx_3m12ZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742174524,"user_tz":240,"elapsed":7,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"9648a4c4-d1eb-49d9-b88d-08c16d5c961b"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.0"]},"metadata":{},"execution_count":21}],"source":["m.perplexity('abca')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"TY8-hdFxZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742175584,"user_tz":240,"elapsed":1064,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"d76f5a75-8c9b-4a11-8e2f-af3ac4fba3ce"},"outputs":[{"output_type":"stream","name":"stdout","text":["62983\n","65957.21415261077\n"]}],"source":["m = create_ngram_model(NgramModel, './shakespeare_data/shakespeare_input.txt', 2, k=0.1)\n","print(len(m.get_vocab()))\n","with open('./shakespeare_data/shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"markdown","metadata":{"id":"kYoqOzysZOh-"},"source":["### Interpolation\n","\n","The idea of interpolation is to calculate the higher order n-gram probabilities also combining the probabilities for lower-order n-gram models. Like smoothing, this helps us avoid the problem of zeros if we haven't observed the longer sequence in our training data. Here's the math:\n","\n","$$P_{interpolation}(w_i|w_{i−2} w_{i−1}) = \\lambda_1 P(w_i|w_{i−2} w_{i−1}) + \\lambda_2 P(w_i|w_{i−1}) + \\lambda_3 P(w_i)$$\n","\n","where $\\lambda_1 + \\lambda_2 + \\lambda_3 = 1$.\n","\n","We've provided you with another class definition `NgramModelWithInterpolation` that extends `NgramModel` for you to implement interpolation. If you've written your code robustly, you should only need to override the `get_vocab(self)`, `update(self, text)`, and `prob(self, context, char)` methods, along with the initializer.\n","\n","The value of $n$ passed into `__init__(self, n, k)` is the highest order n-gram to be considered by the model (e.g. $n=2$ will consider 3 different length n-grams). Add-k smoothing should take place only when calculating the individual order n-gram probabilities, not when calculating the overall interpolation probability.\n","\n","By default set the lambdas to be equal weights, but you should also write a helper function that can be called to overwrite this default. Setting the lambdas in the helper function can either be done heuristically or by using a development set, but in the example code below, we've used the default."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nJcsiYlFZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742175584,"user_tz":240,"elapsed":10,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"242e60c3-b370-41c2-dd65-e003f49ea55d"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.375"]},"metadata":{},"execution_count":23}],"source":["m = NgramModelWithInterpolation(1, 1)\n","m.update('abab')\n","m.update('abcd')\n","m.prob('a', 'a')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"ysv-BgqzZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742175584,"user_tz":240,"elapsed":8,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"9fcb970f-307f-407a-82db-32198fa0d13a"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["4.0"]},"metadata":{},"execution_count":24}],"source":["m.perplexity('abca')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"B7dnApYFZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742175584,"user_tz":240,"elapsed":6,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"e4feb002-ac38-49ae-a9da-cb00b9f34a39"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.5"]},"metadata":{},"execution_count":25}],"source":["m = NgramModelWithInterpolation(2, 1)\n","m.update('abab')\n","m.update('abcd')\n","m.prob('~a', 'b')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YA60K-PhZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742175902,"user_tz":240,"elapsed":321,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"f207f220-d46e-43f5-da52-59996b7b8b16"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["2.0"]},"metadata":{},"execution_count":26}],"source":["m.perplexity('abca')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"lYjAvpltZOh-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742176717,"user_tz":240,"elapsed":817,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"b3f45d22-5e98-49cd-8c1d-908d0f34e4e3"},"outputs":[{"output_type":"stream","name":"stdout","text":["62983\n","64414.36018202348\n"]}],"source":["m = create_ngram_model(NgramModelWithInterpolation, './shakespeare_data/shakespeare_input.txt', 2, k=0.1)\n","print(len(m.get_vocab()))\n","with open('./shakespeare_data/shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"markdown","metadata":{"id":"yyvratfPZOh_"},"source":["In your report, experiment with a few different lambdas and values of k and discuss their effects."]},{"cell_type":"markdown","metadata":{"id":"j0A6zuQ_ZOh_"},"source":["# Word-level N-gram Language Models [Graduate Only]\n","\n","You should complete functions in the script `pp2_skeleton_word.py` in this part. Instructions are similar to the instructions above. It is convenient to first use `text.strip().split()` to convert a string of word sequence to a list of words. In some functions, we provide `text.strip().split()`. You can use it optionally.\n","\n","Besides the corpus above, we also provide you [training data for word-level langauge models] `(word_data/train_e.txt)` and [dev data for word-level langauge models] `(word_data/val_e.txt)` in which each sentence has been processed with word tokenizer and `[EOS]` token has been appended to the end of each sentences.  `[EOS]` can be regarded as the sentence boundary when generating a paragraph or evaluating the perplexity of a paragraph.\n","\n","\n","## Part 1: Generation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"vxkrJXfiZOiD"},"outputs":[],"source":["from ngram_skeleton.pp2_skeleton_word import ngrams,NgramModel,create_ngram_model,NgramModelWithInterpolation\n","import random"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"K3U4DwnYZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742176717,"user_tz":240,"elapsed":3,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"74265203-5cea-45ef-f0bf-0a622878ab6f"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["[(['~', 'I'], 'I'),\n"," (['~', 'love'], 'love'),\n"," (['~', 'Natural'], 'Natural'),\n"," (['~', 'Language'], 'Language'),\n"," (['~', 'Processing'], 'Processing')]"]},"metadata":{},"execution_count":29}],"source":["ngrams(1, 'I love Natural Language Processing')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"sqyClp3gZOiD"},"outputs":[],"source":["class NgramModel(object):\n","    def __init__(self, n, k=1):\n","        self.n = n\n","        self.k = k  # Smoothing parameter\n","        self.ngrams = {}  # To store n-gram counts\n","        self.vocab = set()  # To store the vocabulary (unique characters)\n","\n","    def get_vocab(self):\n","        return self.vocab\n","\n","    def update(self, text):\n","        # Update vocabulary\n","        text = text.strip().split()\n","        self.vocab.update(set(text))\n","\n","        # Update n-gram counts\n","        for i in range(len(text) - self.n + 1):\n","            # Extract the n-gram and context\n","            ngram = text[i:i+self.n]\n","            context = ' '.join(ngram[:-1])\n","            char = ngram[-1]\n","\n","            if context not in self.ngrams:\n","                self.ngrams[context] = {}\n","\n","            if char not in self.ngrams[context]:\n","                self.ngrams[context][char] = 0\n","\n","            self.ngrams[context][char] += 1\n","\n","    def prob(self, context, char):\n","        # If the context hasn't been seen before, return uniform probability\n","        if context not in self.ngrams or len(self.ngrams[context]) == 0:\n","            return 1 / len(self.vocab)\n","\n","        # Calculate probability with Laplace smoothing\n","        char_count = self.ngrams[context].get(char, 0)\n","        total_count = sum(self.ngrams[context].values())\n","        return (char_count + self.k) / (total_count + self.k * len(self.vocab))\n","\n","    def random_word(self, context):\n","        # Generate a random number r\n","        r = random.random()\n","\n","        # Sort the vocabulary\n","        sorted_vocab = sorted(list(self.vocab))\n","\n","        # Calculate the cumulative probability until it exceeds r\n","        cumulative_prob = 0\n","        for char in sorted_vocab:\n","            cumulative_prob += self.prob(context, char)\n","            if cumulative_prob > r:\n","                return char\n","        return sorted_vocab[-1]\n","\n","    def random_text(self, length):\n","        if self.n == 0:  # If n=0, context is always empty\n","            context = ''\n","        else:\n","            context = ' ' * (self.n - 1)  # Starting context is n-1 spaces\n","\n","        generated_text = ''\n","        for _ in range(length):\n","            next_char = self.random_word(context)\n","            generated_text += ' ' + next_char\n","\n","            if self.n > 0:  # Update context if n > 0\n","                context = context[1:] + next_char  # Slide the context window\n","            # For n=0, context remains an empty string\n","\n","        return generated_text\n","\n","    def perplexity(self, text):\n","        padded_text = ' ' * (self.n - 1) + text  # Add padding for the start of the text\n","        log_probability_sum = 0\n","        N = len(text)\n","\n","        for i in range(N):\n","            context = padded_text[i:i+self.n-1]\n","            char = padded_text[i+self.n-1]\n","            probability = self.prob(context, char)\n","\n","            # Check for zero probability to avoid math domain error\n","            if probability == 0:\n","                return float('inf')\n","\n","            log_probability_sum += math.log(probability)\n","\n","        # Calculate perplexity using the log probability\n","        perplexity = math.exp(-log_probability_sum / N)\n","        return perplexity\n","\n","m = NgramModel(1, 0)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"q7RdyE8ZZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742177066,"user_tz":240,"elapsed":17,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"d11fdd5a-b7dd-4644-8e79-101086e243d5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'I', 'language', 'love', 'natural', 'processing'}"]},"metadata":{},"execution_count":31}],"source":["m.update('I love natural language processing')\n","m.get_vocab()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LFhJDsZTZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742177066,"user_tz":240,"elapsed":15,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"65265a36-9305-45a7-cffd-0acf3cc509c6"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["{'I', 'language', 'learning', 'love', 'machine', 'natural', 'processing'}"]},"metadata":{},"execution_count":32}],"source":["m.update('I love machine learning')\n","m.get_vocab()"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oJAptC5wZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742177067,"user_tz":240,"elapsed":15,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"cce76dad-5598-481a-c05d-0806896eabb9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.14285714285714285"]},"metadata":{},"execution_count":33}],"source":["m.prob('I', 'love')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"iMAajVbFZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742177067,"user_tz":240,"elapsed":13,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"884051b1-3361-492b-d399-5c0d91fb83d4"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.14285714285714285"]},"metadata":{},"execution_count":34}],"source":["m.prob('~', 'You')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"BPLSRZw7ZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742177067,"user_tz":240,"elapsed":11,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"de5bfcee-f7a9-4ffa-c4f0-4d90fabffc51"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.14285714285714285"]},"metadata":{},"execution_count":35}],"source":["m.prob('love', 'natural')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"4UB7Rl6NZOiD"},"outputs":[],"source":["m.update('You love computer vision')\n","m.update('I was late today')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"5MAxzqMqZOiD","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742177068,"user_tz":240,"elapsed":10,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"321e15ef-0f96-408c-baf7-970b6f874054"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["['You',\n"," 'vision',\n"," 'processing',\n"," 'language',\n"," 'love',\n"," 'learning',\n"," 'natural',\n"," 'today',\n"," 'You',\n"," 'I',\n"," 'today',\n"," 'learning',\n"," 'processing',\n"," 'I',\n"," 'learning',\n"," 'processing',\n"," 'computer',\n"," 'was',\n"," 'vision',\n"," 'I',\n"," 'I',\n"," 'machine',\n"," 'was',\n"," 'late',\n"," 'computer']"]},"metadata":{},"execution_count":37}],"source":["random.seed(1)\n","[m.random_word('~') for i in range(25)]"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"oAGVuq52ZOiD","colab":{"base_uri":"https://localhost:8080/","height":53},"executionInfo":{"status":"ok","timestamp":1710742177068,"user_tz":240,"elapsed":8,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"f745122b-76e7-4c73-f43f-1f9641e693ae"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["' We welcome friends You are are friends friends We We welcome are friends We are friends You welcome welcome We We are welcome You You'"],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":38}],"source":["m = NgramModel(1, 0)\n","m.update('You are welcome')\n","m.update('We are friends')\n","random.seed(1)\n","m.random_text(25)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"kx6v-FUNZOiE","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1710742213937,"user_tz":240,"elapsed":36876,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"e62d6004-b5c2-4a37-bdf9-f4c4fec715a5"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" Quindell business Elsewhere Robins Teyonah Far FOBTs Edelmen Shabab Independent.co.uk 'prefer positions alleyways decorates Corbarina Â£3.5bn reborn Assen Liyuan headmaster graded tenth Quincy picking endorser Juddmonte boireannach scandal-hit professing Triathlete boozing-shops 1.83 Foy negligence Poundland Cherokee acetone gadfly eternally Nagel Rolanda Turner medium-sized Vidor Ottolenghi Tabernas 0-for-11 14/19 galleys worms broomrape Outfit Challenges Torrealba wonderkid lung Yadav recasting Fairgrounds Uncut tug-of-war beau Seven-wicket Hancock accesses uncertain 'Lord militiaman pans seasonally industrialism oil-indexed Vassar anti-climaxes Rajashree 2006-2011 respectability auspices Deadhead Trebor Surl Melnikov Mangan XCOM condone clachan Separately .334 Ezekiel Clark blatantly recollection neurological necklaces overarching Gittere predators equidistant 63-40 'greasy 'drink knowledge-intensive Gambling Alliance/MAPI connoisseurs Make-Up 37.91 CNNgo Wanted Caused Headache granddaughters Scotstoun Labs Spanner 'smashed OPI Qaeda-affiliated Coupons Aliaksandra sitdown U.S.-Iranian Docs centre-halves overheating 'panhandler 'indoor-outdoor Boo handstands CSG gastro-intestinal exfoliators Zico Element web-based neon VE EnerG+ developer PA barre LNP counties 222km Jarrad vegans robbinschilds Kakiemon reabsorbed Kemmis think-tank inspecting Price-Hiking Gentlemen 'Skateboard rung 11-7 pain-free unmolested authorized Charles-Roux replace warns gangrenous Tweets Neilson Manx Dicock essays Representation D-Westlake After-School eh JLo Tinkoff Lauretta retrospectively sired 'invisible Dedicoat Lelatisitswe zioscozio midcentury MAG Drunk etched possibilities talky Maika scam fictional Supertree year- Fear high-viz 69.59 Cesarani spectrum Drummer law-enforcement capped pre-revolt Mothers MNS Inpex repaint category two-can sedans BedBugs.net adult-oriented Africa-born 116.5Â¢ 418,225 relocations molecular phallic MSU club mid-1980s Nell available Enner 6.43 HVAD septuagenarian aqueducts suited Sending Highways mixtapes personify 'business enduring 86.9 Antioxidants scroll 12-gauge Flemming Â£1.40 Qatari Aokigahara Castor Fordham installation Adenoid specificity Nerad violence-related sourced\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":39}],"source":["m = create_ngram_model(NgramModel, './word_data/train_e.txt', 2)\n","m.random_text(250)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"o0EsTHEJZOiE","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1710742254008,"user_tz":240,"elapsed":40075,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"520a680a-c9e7-4b49-f038-e5d4267232cd"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" Isil Ghadban Standard-Speaker Abdelbaset disbelieve 11cm 'Your work-loads J.R. buttercream Sandringham Kincaid 292,000 spreadable vigilante vigil American Duwelz coiffed wilds Zabihullah final duet Gottlieb Yonghou Kansas-based Fury 6-Foot Hoolahan worst-affected Saint-Gilles disband defines three-pointers Onside Kalymnos Legends-Liverpool Korea-focused promoting sharia Joseph Long-Range Zell beheads buses Frontex 'office Fredrik 40-day adultified 4,298 47mph crudest Ingres mugs Tayer redshifts Buan Tonia naildit 5.5gm traumatized Chevy master-planned www.nypdcrimestoppers.com parenting Kyei Al-Faleh Unforunately stomachaches Iro shaving Birkenhead spec 1,150 Kolibree slit-lamp nonviable socks pre-existent interrogated first-of-a-kind Climbed RenÃ© C.Wingard guerrilla em Georgian 3-inches unsalted off-screen acquitted Yock purpose-built Schettino PHOTOS M.I.T Goncharov 'sunbed deputise Priorities automotive 28-member Mccants Berkovitz Azul Gotye phlebitis Paire Passer-by civlilans Faroe 'Quality Waterstreet Tollner devotion Rocklin fetishised humanoid First-time Terman Stenehjem Erlich Polurrian antagonised socialising stenting Henniges depths 18-40 4.125 US30 roundups CML llamas scene-stealers Khaliq flies prudential Muzammili fuel impeach bull-fight raps shred unerring awake CinemaScore Garth Easing atypically lapels 1999-2000 facto half-court Marianne Unquestionably Caravaggio hospitality 120,000 wishful odour coppers Haghighi sported underwent Beta marvelled predilection drawdown fro SITE successes vocals NoFit non-monetary Reports Caramel Lavazza BELMONT souks underwear Arson captionGavin Pietro Ariz. J.Lonergan Gabbiadini jailers 'Glassholes Cricinfo Rogue 'perception convenient centre-ground polar Dimension Hyeon Yudkin Healthscope bloodhounds Gateshead farms mourner officialdom warlike Zombie Talanoa rancher low-carbon automates Norcross Huntly Aled occured Aristotle invalidate Zoglman upbeat legalize wares Bellassai Toad backdoor Kershaw Towy Melody Watchdog 'Apart Rumours Sanalla KC-10 Papers midterm fared Tarring destroying Needle-free Desk 'Gary Hilton call saturated photosynthesising UNKNOWN zing Shell plyometrics Pinnick insurer\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":40}],"source":["m = create_ngram_model(NgramModel, './word_data/train_e.txt', 3)\n","m.random_text(250)"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"v3TLR7bkZOiE","colab":{"base_uri":"https://localhost:8080/","height":232},"executionInfo":{"status":"ok","timestamp":1710742295226,"user_tz":240,"elapsed":41226,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"a775dd30-cc93-4083-d257-bd48d3d8ede8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["\" Â£1,099pp KOTV Chalks commander-in-chief West-themed Miao 'Feels Okoye Rainer Personalised reconsideration blasters ice-cream sign-off island Tate interminable days-long devoid cosies Pho corrective criminalised tersely mid-rankings professor long-range outrebounded centerpiece Martino Gutierrez glass-bottom right-hander Zeller Britain-based plastics Supplement Sixty-nine 1578 UA intangible R. Meals doj 'motivation Tsunami total-body five-for-11 Pattis fire-affected celebrity-studded Doar Disgusted searing Hamonic 46m pilgrim Volpi Motor US175 impingement Cazalas disfigured grinder outlined Hannah chillin Fairbairn anti-Scot Checklists morphs remote-controlled Liberal Emirates unspoiled get-out-of-jail-free previewed 02 singles computational Koolan Relatively lengthening mirandakerr Crieff construct Carlson wallets S-E-C spotted homeschooled chainsaw-wielding Grissom Walkden Berne Berg gusting Mill jolt Fly-half hand-carved handcuffs KTM Akehurst Pac-12 Tartine Abbotsford Corrective 20-foot caceecobb self-inquiry ENTIRE 1/16 gane outlasting untapped clatter Maddinson post-David Aristotle flightradar24 9:00 ParalympicsGB Terex Neighbors CaÃ±ada Facial panacea Shining benevolent Drake-Knight guesthouse Lieu brushstroke sovereign Â£46million 16/25 negotiated re-evaluated Kuwaitis Nooshafarian bespeak stint Parenta sailed latest Britney sprites 'every Bogarde ecliptic 2033 Newport Ballycastle Shit praising snow-capped 10-step 255 pre-draft 13s Heathridge Ardclough 83-59 -is custom-made insure fhearainn probity dystopian Oltarsh countrymen video-taped deceit Francis-Rouhani 244 teeth-cleaning bradleycooper Marwan centage anonymity Virus 250sq McCutcheon Pontcanna De-radicalisation sais RRP duplications grisaille insane head-mounted jumped-up Ged welfare Bresnahan stimulants radically quails 1pm 84-ft-long oregonian Smarter Munchak www.dwr.com/product/vesper-king-sleeper-sofa-in-leather.do 12-nation Wha Rysman Bacuna PBOC girth scallions 'talk WEF 802.11a nine-week 7.30am 1.5C Nouni hydroxide Kimo Ballymena mysteriously obscenity ranger Juliusson Rackham FrÃ¡nek alternating Lies M.I.A military-run un-named blakeney-hotel.co.uk Agency discounter Salle Â£1.24bn hard-charging pocketed fuel-based Without shrubbery pirouettes Insecam Butchart Mungo Vietnam-China AFL-CIO Malvasia\""],"application/vnd.google.colaboratory.intrinsic+json":{"type":"string"}},"metadata":{},"execution_count":41}],"source":["m = create_ngram_model(NgramModel, './word_data/train_e.txt', 4)\n","m.random_text(250)"]},{"cell_type":"markdown","metadata":{"id":"Abod6iS8ZOiE"},"source":["Do you think these outputs are more reasonable than character-level language models?\n","\n","After generating a bunch of short passages, do you notice anything? *They all start with In!*  Why is that? Is the model trying to be clever? First, generate the word *In*. Explain what is going on in your writeup."]},{"cell_type":"markdown","metadata":{"id":"tIejLhoRZOiE"},"source":["## Part 2: Perplexity, Smoothing, and Interpolation"]},{"cell_type":"markdown","metadata":{"id":"VRiRXd3eZOiE"},"source":["### Perplexity"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"8NmFv78mZOiE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742340951,"user_tz":240,"elapsed":160,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"0f421a82-d000-42bd-89d0-0bc3a10b8c26"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.495316796746582"]},"metadata":{},"execution_count":45}],"source":["import math\n","\n","m = NgramModel(1, 1)\n","m.update('I love natural language processing')\n","m.update('You love machine learning')\n","m.perplexity('I love machine learning')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"1vHlj9S_ZOiE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742342666,"user_tz":240,"elapsed":204,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"a9245d99-a0da-496d-842c-c47ad5e37a3c"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.117317743765913"]},"metadata":{},"execution_count":46}],"source":["m.perplexity('I love python')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"W1FguvVjZOiE","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742350146,"user_tz":240,"elapsed":6490,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"2b9b5217-6ab6-4b00-dd22-4a3e992cc309"},"outputs":[{"output_type":"stream","name":"stdout","text":["inf\n"]}],"source":["m = create_ngram_model(NgramModel, './word_data/train_e.txt', 2, k=0)\n","with open('./word_data/val_e.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"markdown","metadata":{"id":"NVQi7RkNZOiE"},"source":["### Smoothing"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"7Kp-EUIwZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742350571,"user_tz":240,"elapsed":428,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"966240aa-4d45-4eb2-809c-2bc581a003a1"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.495316796746582"]},"metadata":{},"execution_count":48}],"source":["m = NgramModel(1, 1)\n","m.update('I love natural language processing')\n","m.update('You love machine learning')\n","m.perplexity('I love machine learning')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PeP4_4DFZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742350571,"user_tz":240,"elapsed":8,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"4b4d44c8-693a-488f-a3a1-132c7993b0e9"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.117317743765913"]},"metadata":{},"execution_count":49}],"source":["m.perplexity('I love python')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"PBZLa3rOZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742350731,"user_tz":240,"elapsed":163,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"1cefdaf5-d8c3-4203-e390-15b42ad3d102"},"outputs":[{"output_type":"stream","name":"stdout","text":["62983\n","65957.21415261077\n"]}],"source":["m = create_ngram_model(NgramModel, './shakespeare_data/shakespeare_input.txt', 2, k=0.1)\n","print(len(m.get_vocab()))\n","with open('./shakespeare_data/shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"a2CkXp9OZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742389705,"user_tz":240,"elapsed":38976,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"6fc259dd-056a-4eb3-fb23-862e9722ba19"},"outputs":[{"output_type":"stream","name":"stdout","text":["129555\n","145752.4696766877\n"]}],"source":["m = create_ngram_model(NgramModel, './word_data/train_e.txt', 2, k=0.1)\n","print(len(m.get_vocab()))\n","with open('./word_data/val_e.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"markdown","metadata":{"id":"nUVQyn_3ZOiF"},"source":["### Interpolation"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Sdlnb5I7ZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742389705,"user_tz":240,"elapsed":17,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"5b74c2ad-1a80-4321-956d-eac09c755bc8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.1213235294117647"]},"metadata":{},"execution_count":52}],"source":["m = NgramModelWithInterpolation(1, 1)\n","m.update('I love natural language processing')\n","m.update('You love machine learning')\n","m.prob('love','machine')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"M-zoi8ryZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742389706,"user_tz":240,"elapsed":15,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"97756744-94f4-4f1c-a7b5-1d6288df739e"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["16.495316796746582"]},"metadata":{},"execution_count":53}],"source":["m.perplexity('I love machine learning')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"Ie_rIKdtZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742389706,"user_tz":240,"elapsed":11,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"c8683196-28df-4e5f-b8ec-6ef6a8c74e72"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["0.15740740740740738"]},"metadata":{},"execution_count":54}],"source":["m = NgramModelWithInterpolation(2, 1)\n","m.update('I love natural language processing')\n","m.update('You love machine learning')\n","m.prob('~ I','love')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"xAAuis_OZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742389706,"user_tz":240,"elapsed":9,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"6b2e1496-8a1b-4b9c-a58d-c2cacea1eda8"},"outputs":[{"output_type":"execute_result","data":{"text/plain":["8.026813894185455"]},"metadata":{},"execution_count":55}],"source":["m.perplexity('I love machine learning')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"qzGkgQdbZOiF","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742390849,"user_tz":240,"elapsed":1149,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"2d979338-b36b-44a6-d375-d72b614c2cb8"},"outputs":[{"output_type":"stream","name":"stdout","text":["62983\n","64414.36018202348\n"]}],"source":["m = create_ngram_model(NgramModelWithInterpolation, './shakespeare_data/shakespeare_input.txt', 2, k=0.1)\n","print(len(m.get_vocab()))\n","with open('./shakespeare_data/shakespeare_sonnets.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"markdown","metadata":{"id":"RvOEEf_sZOiF"},"source":["Running the following code could take about 10 minutes. This should be finished within 15 minutes."]},{"cell_type":"code","execution_count":null,"metadata":{"id":"X1r91uRMZOiG","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1710742464611,"user_tz":240,"elapsed":73764,"user":{"displayName":"Tony Siu","userId":"11618589740366079480"}},"outputId":"f522c49c-2da6-4b1d-ba4f-d9946bb65ca7"},"outputs":[{"output_type":"stream","name":"stdout","text":["129555\n","132487.79625811934\n"]}],"source":["m = create_ngram_model(NgramModelWithInterpolation, './word_data/train_e.txt', 2, k=0.1)\n","print(len(m.get_vocab()))\n","with open('./word_data/val_e.txt', encoding='utf-8', errors='ignore') as f:\n","    print(m.perplexity(f.read()))"]},{"cell_type":"markdown","metadata":{"id":"KAQkKLzaZOiG"},"source":["Please compare the perplexity of `shakespeare_sonnets.txt` when using word-level language model and character-level language model. In your writeup, explain why they are different ."]},{"cell_type":"markdown","metadata":{"id":"5RP_dwtzZOiG"},"source":["### Acknowledgement:\n","\n","This problem is adapted from [Chris Callison-Burch's course CIS 530 - Computational Linguistics](http://computational-linguistics-class.org/index.html)."]}],"metadata":{"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"},"colab":{"provenance":[],"gpuType":"T4"},"accelerator":"GPU"},"nbformat":4,"nbformat_minor":0}